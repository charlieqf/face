# LLM 驱动音画同步 (Voice + Expression) 技术调研报告

针对“基于 LLM 生成文本，同时进行语音输出与面部表情”的场景，以下是基于 `Voice_Driven_Humanoid_Head` 和 `OpenRoboExp` 的深度参考价值分析。

## 1. 核心技术架构方案

### 方案 A：预生成式同步 (高表现力，参考 OpenRoboExp)
这种方案类似于“对口型”，效果最自然。
- **流程**：`LLM 文本 -> TTS 生成音频文件 -> 提取音频特征 (Wav2Vec2) -> 模型预测表情序列 -> 同时播放音频与序列`。
- **参考价值**：
    - **音频特征提取**：使用 `OpenRoboExp` 中的核心代码，将音频转为 25fps 的特征向量。
    - **平滑算法**：借鉴其低通滤波 (Butterworth Filter)，确保电机动作不会因为模型抖动。
- **优点**：表情与发音完全匹配，且可以预先计算出最平滑的动作。

### 方案 B：实时触发式同步 (低延迟，针对 LLM 流式输出)
这种方案适合 LLM 边说话边生成的场景。
- **流程**：`LLM 文本流 -> 逐词/逐句识别关键词 -> 触发预设表情指令 -> TTS 流式播放`。
- **参考价值**：
    - **关键词匹配**：参考 `Voice_Driven_Humanoid_Head` 的 `Decision.py`，根据文本中的关键词（如“开心”、“思考”）立即改变眼球和眼皮状态。
- **优点**：响应速度快，适合实时会话。

## 2. 三电机场景的特殊适配

由于我们只有 **嘴、眼球、眼皮**，不需要像 `OpenRoboExp` 那样预测 52 个参数。我们可采取以下简化策略：

1. **嘴部同步**：
    - 参考 `Voice_Driven`：根据 TTS 播放状态简单开合。
    - **进阶**：从音频包络 (Envelope) 提取音量强度，直接映射到嘴巴 1-5 档。
2. **情绪建模**：
    - 根据 LLM 返回的文本，由一个小型情感分类模型预测当前应处的“表情基调”（如：微笑、严肃）。
3. **随机微动**：
    - 参考 `OpenRoboExp` 的数据增强理念，即使没有说话，也要给眼球和眼皮加入随机的微小偏移值，避免“死气沉沉”。

## 3. 对本项目的具体建议

- **数据采集**：在 `generate_dataset_v2.py` 中，不应只生成随机数，而应模拟“语音强度”对嘴部的影响。
- **模型设计**：我们的 MLOps 管道应支持输入一段文本序列，输出一段对应的 (mouth, eye, lid) 指令流。
- **中间协议**：建议使用 `Time-stamped Sequence`。

---
> [!IMPORTANT]
> **结论**：我们将走一条“**文本情感引导 + 音频包络驱动**”的混合路径。即：用 LLM 文本决定眼球和眼皮的“神态”，用音频的能量值实时驱动嘴巴的“动作”。
