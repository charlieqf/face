# 外部仓库调研报告：Voice_Driven_Humanoid_Head & OpenRoboExp

通过对两个核心开源项目的研究，以下是可用于我们项目的关键技术点和参考价值。

## 1. Voice_Driven_Humanoid_Head
**核心价值**：完整的交互式闭环流程。

- **流程参考**：`Whisper (语音转文字) -> LLM (逻辑决策) -> TTS (语音合成) -> 舵机驱动`。
- **硬件控制**：它将机器人头视为一个消息节点（使用 Igescape 框架）。
- **启发**：我们的“语音演示适配器”可以参考其 `SpeechTranscription` 和 `Decision` 的模块化分工，确保系统可插拔。

## 2. OpenRoboExp
**核心价值**：高精度的表情映射与时序平滑算法。

- **算法参考**：
    - **Wav2Vec2**：使用预训练的语音模型提取特征，直接预测表情参数。这与我们的 MLOps 进化方向高度一致。
    - **时序平滑（关键）**：使用 `butter_lowpass_filter` (巴特沃斯低通滤波器)。
        - *意义*：解决机器人动作抖动、僵硬的问题，让表情看起来有“惯性”和“肉感”。
- **映射契约**：使用 52 个标准的 Apple ARKit Blendshapes 作为中间层。
    - *意义*：这样训练出的模型具有通用性，不仅能控制机器人，也能控制虚拟人。

## 3. 对本项目的建议改进

1. **引入平滑层**：在 `robot_pipeline.py` 的推理步骤后增加一个低通滤波步骤。
2. **中间层标准化**：虽然我们只有 3 个电机，但建议模型内部先预测 ARKit 关键参数（如 `jawOpen`），再映射到我们硬件的 1-5 档位。
3. **Web 模拟器设计**：参考其表情序列的存储格式（CSV），我们的模拟器应支持加载带有时序（FPS）的序列文件。

---
> [!TIP]
> **技术决策**：我们将借鉴 `OpenRoboExp` 的数据时序处理逻辑，确保机器人的动作不仅是“对的”，而且是“顺滑的”。
