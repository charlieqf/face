# 机器人表情映射项目文档 (Robot Expression Mapping)

本项目旨在建立一个从“文字语义”到“机器人面部动作”的自动化 MLOps 管道。主要用于“机器人直播”场景，使机器人能够根据直播文案实时产生自然、平滑的表情。

## 1. 核心模型架构

项目采用从简到繁的模型演进策略：

### 1.1 基线模型 (Random Forest)
- **类型**：多输出回归随机森林 (Multi-output Random Forest)。
- **用途**：作为 PoC 阶段模型，快速验证从字符特征到 (Mouth, Eye, Lid) 三元组的映射。
- **优点**：无需复杂的特征缩放，鲁棒性高。

### 1.2 进阶模型 (Bi-LSTM)
- **类型**：双向长短期记忆网络 (Bidirectional LSTM)。
- **用途**：正式直播版本。
- **逻辑**：利用其时序记忆能力，捕捉文字前后文的情感连贯性。模型采用**按句处理 (Batch Processing)** 模式，即等待 LLM 生成完完整的一句话后，一致性预测该句的表情序列，以确保表现的连贯与自然。

---

## 2. 操作指南 (Execution Guide)

### 2.1 手动执行脚本 (Manual Scripts)

用于数据准备和效果预览。

| 任务 | 命令行示例 | 说明 |
| :--- | :--- | :--- |
| **生成数据集** | `python scripts/generate_dataset_v2.py` | **原理与局限性解析：**<br>1. **生成原理**：基于“语义-情绪映射表”。脚本预设了正向、负向、中性、惊讶四种情感规则库，将特定的词库映射到电机的目标值区间，并引入了随机高斯噪声来模拟传感器采集时的抖动（Jitter）。<br>2. **局限性**：数据属于“合成数据”（Synthetic Data），缺乏真实面部感知（如 sEMG 信号）的非线性复杂性；且目前仅按字符步进，未考虑真实发音的语音时长差异。 |
| **可视化模拟器** | 在浏览器打开 `src/simulator/index.html` | **原理与预期效果：**<br>1. **实现原理**：通过 JavaScript 实时解析 `prediction.json` 指令流，将离散的挡位值（如眼球 1-21）映射为 CSS 的 `transform` 和 `top/height` 坐标。利用 CSS3 的 `transition` 属性实现类似伺服电机的线性运动平滑感。<br>2. **预期效果**：提供低延迟、高保真的动作预览，动态展示机器人“直播”时的表情切换。<br>3. **局限性**：由于浏览器渲染引擎限制，在高负荷下可能存在极微小的时间偏移；且 CSS 动画仅能模拟线性平滑，无法完全还原真实舵机的重力加速度与震动细节。 |
| **手工播放测试** | `python scripts/expression_player.py` | **原理与预期效果：**<br>1. **核心原理**：作为底层直接驱动工具，它调用 `RobotController` 抽象层，将 CSV/JSON 表情序列逐帧转换为硬件 TTL 指令（或 Mock 文本），并利用 `time.sleep` 控制播放步长。<br>2. **预期效果**：通过指令级追踪，验证表情三元组在虚拟或实际串口上的通信正确性。 |

### 2.2 MLOps 自动化脚本 (ZenML & MLflow)

用于模型训练、评估和版本管理。

| 任务 | 具体操作步骤 | 预期在页面/终端看到的内容 |
| :--- | :--- | :--- |
| **运行训练管道** | 在终端输入：<br>`python pipelines/robot_pipeline.py` | **ZenML 流程进度**：<br>1. 看到终端出现绿色的进度条，依次显示 Step `loader` -> `trainer` -> `evaluator`。<br>2. 最终会显示 `Pipeline run has finished successfully!`。<br>3. 脚本会自动在 `data/output/` 下更新 `prediction.json` ファイル。 |
| **查看实验报告** | 1. 在终端输入：`mlflow ui`<br>2. 打开浏览器访问：`http://127.0.0.1:5000` | **MLflow 画布内容**：<br>1. **Runs 列表**：每一行代表一次训练尝试，你可以点击进入详情。<br>2. **Metrics (指标)**：核心看 `test_mse` (测试集均方误差)。详见下方【名词解析】。<br>3. **Parameters (参数)**：记录了你当时用的配置，方便找回“最好的参数”。<br>4. **Artifacts (产出物)**：可以看到被保存起来的模型文件和训练曲线图。 |

---

## 3. 核心名词科普 (Terms for Non-Experts)

### 3.1 什么是面部表情模型的 MSE？
**MSE (Mean Squared Error, 均方误差)** 在本项目中衡量的是：**“模型预测的挡位”与“传感器记录的真实挡位”之间的物理距离**。
- **直观理解**：如果传感器记录眼球该在 11 档，模型预测在 10 档，误差就是 1。MSE 会把这些误差平方后求平均。
- **数值意义**：MSE 越接近 0，表示机器人的表情还原度越高，动作越接近真实的人类/传感器标注。

### 3.2 训练集 vs 测试集 (训练数据 vs “闭卷考试”)
- **训练集 (Train Set)**：模型用来“背书”的数据。如果训练集 MSE 很高，说明模型根本没学会。
- **测试集 (Test Set)**：模型从未见过的“新句子”。这相当于对模型进行**闭卷考试**。
- **为什么重要**：在机器人直播中，LLM 会产生全新的文本。只有在测试集上 MSE 表现优秀，才能证明机器人有处理“陌生文案”并做出正确表情的能力。
| **数据版本回溯** | 1. 看到 `data/raw/` 报红或缺失时。<br>2. 运行：`dvc pull` | **DVC 自动管理**：<br>1. 它会像 Git 一样把之前备份在本地或云端的大文件“拉取”回来。<br>2. 确保你和团队成员用的训练数据是完全一致的，不会因为数据变动导致结果对不上。 |

---

## 4. 数据契约与存储 (Data Management)

| 文件路径 | 类型 | 用途 / 描述 |
| :--- | :--- | :--- |
| `data/raw/training_data.csv` | **输入数据** | **单一大文件**。为了方便 ZenML 管道一次性读取，我们将所有的直播样本序列（例如 2000 条句子）合并存储在这个 CSV 中。每一行代表一个字符及其对应的表情状态，通过 `sequence_id` 来区分不同的句子。 |
| `configs/train_config.yaml` | **配置参数** | 定义训练步长、学习率、模型选择等，实现“训练过程可配置”。 |
| `data/output/prediction.json` | **推理契约** | 模型预测的动作流。格式为包含 `mouth`, `eye`, `lid`, `duration` 的 JSON 数组。 |
| `models/` | **产出模型** | 存放训练好的序列化模型文件（由 MLflow 自动存档）。 |

---

## 5. 关键算法流程

1. **文本向量化**：将直播文案转换为词嵌入向量。
2. **时序建模**：LSTM 预测随时间变化的表情三元组。
3. **后处理平滑与量化**：
    - 应用 **巴特沃斯低通滤波器** 过滤模型预测的阶梯效应。
    - **量化 (Rounding)**：平滑处理后的小数将四舍五入为离散挡位（如 3.2 -> 3），以符合电机控制协议。
4. **加载演示**：模拟器读取 `prediction.json`，按时间戳驱动 CSS 动画。

---
> [!NOTE]
> 本项目的 MLOps 核心在于：**让每一次的表情改进都可复现、可追踪、可对比。**
