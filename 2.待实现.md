# 项目路线图：迈向真实训练与直播 (待实现清单)

目前项目已完成 MLOps 闭环原型与模拟验证。要进化到真实的机器人直播与工业级训练，仍需完成以下核心模块：

---

## 1. 硬件与数据采集层 (Data & Hardware)

- [ ] **物理串口集成 (Hardware Interface)**：
    - **逻辑切换**：修改 `src/hardware/serial_port.py`，将目前的 `SerialPortMock` 替换为基于 `pyserial` 库的真实串口通信代码。
    - **环境适配**：需适配 Windows 11 下的 `COM` 端口识别逻辑。
    - **时序压力测试**：验证在真实串行总线下的指令响应延迟与稳定性。
- [ ] **真实传感器数据采集 (Real-word Data)**：
    - **采集端**：开发数据采集工具，同步记录“演示文案”与“人工标注/传感器回传（如 sEMG 肌肉信号、ARKit 面部动捕）”。
    - **对齐**：通过 DVC 管理这些高维度、非线性的真实标注数据。
- [ ] **电机标定 (Calibration)**：
    - 建立一套映射工具，将模型输出的 1-21 虚拟挡位与不同品牌舵机的实际 PWM 脉宽或角度精准对应。

---

## 2. 模型与同步算法层 (AI & Sync)

- [ ] **时序模型升级 (Bi-LSTM / GRU)**：
    - 从目前的“单字符”随机森林升级为能够处理“长句上下文”的时序模型，确保表情在语义停顿处的自然过渡。
- [ ] **TTS 时间戳闭环 (Lip-Sync)**：
    - 接入真实 TTS 引擎（如 Azure, Edge-TTS）。
    - 捕获 TTS 吐出的单个字符精确时间偏移量，动态调整 `prediction.json` 中每一帧的 `duration`。
- [ ] **低通滤波稳健化**：
    - 将巴特沃斯滤波集成到推理管道末端，模拟肌肉的物理加速和阻尼感，消除“瞬移”。

---

## 3. LLM 智脑与互动闭环 (Brain & Interaction)

- [ ] **LLM API 集成 (OpenAI / Gemini / Grok)**：
    - 开发底层对接模块，支持流式 (Streaming) 获取直播文案或观众提问的回答。
- [ ] **提示词工程 (Prompt Engineering)**：
    - 设计机器人专属的 **System Prompt**，使其不仅能回答问题，还能在文本中包含“情感标记”（如 `[惊讶] 真的吗？`），辅助模型进行更精准的表情映射。
- [ ] **全流程互动循环**：
    - **逻辑流**：观众语音/文字提问 -> ASR (转文字) -> LLM (生成回答) -> **本项目系统 (表情映射)** -> TTS (发音) -> 驱动硬件。
- [ ] **短时记忆与个性**：
    - 集成 RAG 或上下文管理，使机器人在直播中能记住“刚才那位观众”的话，并带有连贯的性格色彩。

## 4. 实时直播工程层 (Streaming Engineering)

- [ ] **流式推理架构**：
    - 虽然用户要求“按句预测”，但为了直播响应速度，需要建立“预取缓存 (Prefetching buffer)”：LLM 生成一句话 -> 模型后台预测表情 -> TTS 开始预热 -> 整体播放。
- [ ] **异常处理机制**：
    - 建立硬件通信超时重试、表情越界截断等保护机制。

---

## 5. MLOps 工业化增强

- [ ] **部署端监测 (Monitoring)**：
    - 在直播过程中记录推理失败率、硬件指令执行延迟。
- [ ] **数据回流 (Active Learning)**：
    - 记录直播过程中表现不佳的表情，通过 DVC 回流到数据中心，进行增量训练。

---
> [!NOTE]
> **当前优先级**：建议下一步优先攻克 **“1. 物理串口集成”** 与 **“2. TTS 时间戳对接”**，这是打破虚拟与现实边界的关键。
